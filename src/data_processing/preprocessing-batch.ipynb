{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing large corpus into batchs for relation extraction\n",
    "\n",
    "- batchfy fids and save each batch into a directry under main direction\n",
    "- generate tsv files under each direction for as test.tsv\n",
    "- maintain a tracking file\n",
    "- consider how to merge the results in postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## you need to update all file paths accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle as pkl\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import permutations, combinations\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pkl_save(data, file):\n",
    "    with open(file, \"wb\") as f:\n",
    "        pkl.dump(data, f)\n",
    "\n",
    "        \n",
    "def pkl_load(file):\n",
    "    with open(file, \"rb\") as f:\n",
    "        data = pkl.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_text(ifn):\n",
    "    with open(ifn, \"r\") as f:\n",
    "        txt = f.read()\n",
    "    return txt\n",
    "\n",
    "\n",
    "def save_text(text, ofn):\n",
    "    with open(ofn, \"w\") as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# https://github.com/uf-hobi-informatics-lab/NLPreprocessing (git clone this repo to local)\n",
    "# sys.path.append(\"path to /NLPpreprocessing\")\n",
    "# sys.path.append(\"path to /NLPreprocessing/text_process\")\n",
    "sys.path.append(\"./NLPpreprocessing\")\n",
    "sys.path.append(\"./NLPpreprocessing/text_process/\")\n",
    "from annotation2BIO import pre_processing, read_annotation_brat, generate_BIO\n",
    "MIMICIII_PATTERN = \"\\[\\*\\*|\\*\\*\\]\"\n",
    "from sentence_tokenization import logger as l1\n",
    "from annotation2BIO import logger as l2\n",
    "l1.disabled = True\n",
    "l2.disabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_entity_to_sent_mapping(nnsents, entities, idx2e):\n",
    "    loc_ens = []\n",
    "    \n",
    "    ll = len(nnsents)\n",
    "    mapping = defaultdict(list)\n",
    "    for idx, each in enumerate(entities):\n",
    "        en_label = idx2e[idx]\n",
    "        en_s = each[2][0]\n",
    "        en_e = each[2][1]\n",
    "        new_en = []\n",
    "        \n",
    "        i = 0\n",
    "        while i < ll and nnsents[i][1][0] < en_s:\n",
    "            i += 1\n",
    "        s_s = nnsents[i][1][0]\n",
    "        s_e = nnsents[i][1][1]\n",
    "\n",
    "        if en_s == s_s:\n",
    "            mapping[en_label].append(i)\n",
    "\n",
    "            while i < ll and s_e < en_e:\n",
    "                i += 1\n",
    "                s_e = nnsents[i][1][1]\n",
    "            if s_e == en_e:\n",
    "                 mapping[en_label].append(i)\n",
    "            else:\n",
    "                mapping[en_label].append(i)\n",
    "                print(\"last index not match \", each)\n",
    "        else:\n",
    "            mapping[en_label].append(i)\n",
    "            print(\"first index not match \", each)\n",
    "\n",
    "            while i < ll and s_e < en_e:\n",
    "                i += 1\n",
    "                s_e = nnsents[i][1][1]\n",
    "            if s_e == en_e:\n",
    "                 mapping[en_label].append(i)\n",
    "            else:\n",
    "                mapping[en_label].append(i)\n",
    "                print(\"last index not match \", each)\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def get_permutated_relation_pairs(eid2idx):\n",
    "    all_pairs = []\n",
    "    all_ids = [k for k, v in eid2idx.items()]\n",
    "    for e1, e2 in permutations(all_ids, 2):\n",
    "        all_pairs.append((e1, e2))\n",
    "    return all_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "procedure:\n",
    "\n",
    "1. preprocess text into sentences\n",
    "2. find each entity associated sentence idx\n",
    "3. generate entity pairs as relation candidates\n",
    "4. extract eneity associated sentences, locate entities in pair and insert special tags\n",
    "5. save generated data\n",
    "\n",
    "result output:\n",
    "\n",
    "1. keep pos predicted relations\n",
    "2. using map files to locate relation associated entities\n",
    "3. output as brat\n",
    "\"\"\"\n",
    "def validate_rels(rels, valid):\n",
    "    nrels = []\n",
    "    for rel in rels:\n",
    "        rtype = rel[0]\n",
    "        if tuple(rtype) not in valid:\n",
    "            print(\"invalid: \", rel)\n",
    "            continue\n",
    "        nrels.append(rel)\n",
    "    return nrels\n",
    "\n",
    "\n",
    "def check_tags(s1, s2):\n",
    "    assert EN1_START in s1 and EN1_END in s1, f\"tag error: {s1}\"\n",
    "    assert EN2_START in s2 and EN2_END in s2, f\"tag error: {s2}\"\n",
    "\n",
    "\n",
    "def format_relen(en, rloc, nsents):\n",
    "    if rloc == 1:\n",
    "        spec1, spec2 = EN1_START, EN1_END\n",
    "    else:\n",
    "        spec1, spec2 = EN2_START, EN2_END\n",
    "    sn1, tn1 = en[0][3]\n",
    "    sn2, tn2 = en[-1][3]\n",
    "    target_sent = nsents[sn1]\n",
    "    target_sent = [each[0] for each in target_sent]\n",
    "    ors =  \" \".join(target_sent)\n",
    "    \n",
    "    if sn1 != sn2:\n",
    "#         print(\"[!!!Warning] The entity is not in the same sentence\\n\", en)\n",
    "        tt = nsents[sn2]\n",
    "        tt = [each[0] for each in tt]\n",
    "        target_sent.insert(tn1, spec1)\n",
    "        tt.insert(tn2+1, spec2)\n",
    "        target_sent = target_sent + tt\n",
    "#         print(target_sent)\n",
    "    else:\n",
    "        target_sent.insert(tn1, spec1)\n",
    "        target_sent.insert(tn2+2, spec2)\n",
    "    \n",
    "    fs = \" \".join(target_sent)\n",
    "    \n",
    "    return sn1, sn2, fs, ors\n",
    "\n",
    "\n",
    "def gene_true_relations(rels, mappings, ens, e2i, nnsents, nsents, valid_comb, fid=None):\n",
    "    true_pairs = set()\n",
    "    pos_samples = []\n",
    "    \n",
    "    for rel in rels:\n",
    "        rel_type = rel[0]\n",
    "        enid1, enid2 = rel[1:]\n",
    "        \"\"\"\n",
    "        [['100', (15443, 15446), (16473, 16476), (231, 4), 'B-Strength'], \n",
    "        ['mg', (15447, 15449), (16477, 16479), (231, 5), 'I-Strength']] \n",
    "        [['Metoprolol', (15422, 15432), (16452, 16462), (231, 2), 'B-Drug'], \n",
    "        ['Succinate', (15433, 15442), (16463, 16472), (231, 3), 'I-Drug']]\n",
    "        \"\"\"\n",
    "        enbs1, enbe1 = mappings[enid1]\n",
    "        en1 = nnsents[enbs1: enbe1+1]\n",
    "        si1, sii1, fs1, ors1 = format_relen(en1, 1, nsents)\n",
    "        enbs2, enbe2 = mappings[enid2]\n",
    "        en2 = nnsents[enbs2: enbe2+1]\n",
    "        si2, sii2, fs2, ors2 = format_relen(en2, 2, nsents)\n",
    "        sent_diff = abs(si1 - si2)\n",
    "        \n",
    "        en1t = en1[0][-1].split(\"-\")[-1]\n",
    "        en2t = en2[0][-1].split(\"-\")[-1]\n",
    "\n",
    "        true_pairs.add((enid1, enid2))\n",
    "        \n",
    "        if (en1t, en2t) not in valid_comb:\n",
    "            continue\n",
    "        \n",
    "        if sent_diff <= CUTOFF:\n",
    "            check_tags(fs1, fs2)\n",
    "            assert (en1t, en2t) in valid_comb, f\"{en1t} {en2t}\"\n",
    "            if DO_BIN:\n",
    "                pos_samples.append((sent_diff, \"pos\", fs1, fs2, en1t, en2t, enid1, enid2, fid))\n",
    "            else:\n",
    "                pos_samples.append((sent_diff, rel_type, fs1, fs2, en1t, en2t, enid1, enid2, fid))\n",
    "\n",
    "    return pos_samples, true_pairs\n",
    "        \n",
    "\n",
    "def gene_neg_relation(perm_pairs, true_pairs, mappings, ens, e2i, nnsents, nsents, valid_comb, fid=None):\n",
    "    neg_samples = []\n",
    "    for each in perm_pairs:\n",
    "        enid1, enid2 = each\n",
    "        \n",
    "        # not in true relation\n",
    "        if (enid1, enid2) in true_pairs:\n",
    "            continue\n",
    "        \n",
    "        enc1 = ens[e2i[enid1]]\n",
    "        enc2 = ens[e2i[enid2]]\n",
    "\n",
    "        enbs1, enbe1 = mappings[enid1]\n",
    "        en1 = nnsents[enbs1: enbe1+1]\n",
    "        si1, sii1, fs1, ors1 = format_relen(en1, 1, nsents)\n",
    "        enbs2, enbe2 = mappings[enid2]\n",
    "        en2 = nnsents[enbs2: enbe2+1]\n",
    "        si2, sii2, fs2, ors2 = format_relen(en2, 2, nsents)\n",
    "        sent_diff = abs(si1 - si2)\n",
    "        \n",
    "        en1t = en1[0][-1].split(\"-\")[-1]\n",
    "        en2t = en2[0][-1].split(\"-\")[-1]\n",
    "        \n",
    "        if (en1t, en2t) not in valid_comb:\n",
    "            continue\n",
    "        \n",
    "        if sent_diff <= CUTOFF:\n",
    "            check_tags(fs1, fs2)\n",
    "            assert (en1t, en2t) in valid_comb, f\"{en1t} {en2t}\"\n",
    "            if fid:\n",
    "                neg_samples.append((sent_diff, NEG_REL, fs1, fs2, en1t, en2t, enid1, enid2, fid))\n",
    "            else:\n",
    "                neg_samples.append((sent_diff, NEG_REL, fs1, fs2, en1t, en2t, enid1, enid2))\n",
    "    \n",
    "    return neg_samples\n",
    "\n",
    "\n",
    "def create_test_samples(file_path, valids=None, valid_comb=None):\n",
    "    #create a separate mapping file\n",
    "    rel_mappings = []\n",
    "    #\n",
    "    fids = []\n",
    "    root = Path(file_path)\n",
    "    preds = defaultdict(list)\n",
    "    \n",
    "    for txt_fn in root.glob(\"*.txt\"):\n",
    "        fids.append(txt_fn.stem)\n",
    "        ann_fn = root / (txt_fn.stem + \".ann\")\n",
    "\n",
    "        # load text\n",
    "        txt = load_text(txt_fn)\n",
    "        pre_txt, sents = pre_processing(txt_fn, deid_pattern=MIMICIII_PATTERN)\n",
    "        e2i, ens, _ = read_annotation_brat(ann_fn)\n",
    "        i2e = {v: k for k, v in e2i.items()}\n",
    "        \n",
    "        nsents, sent_bound = generate_BIO(sents, ens, file_id=\"\", no_overlap=False, record_pos=True)\n",
    "        total_len = len(nsents)\n",
    "        nnsents = [w for sent in nsents for w in sent]\n",
    "        mappings = create_entity_to_sent_mapping(nnsents, ens, i2e)\n",
    "        \n",
    "        perm_pairs = get_permutated_relation_pairs(e2i)\n",
    "        pred = gene_neg_relation(perm_pairs, set(), mappings, ens, e2i, nnsents, nsents, valid_comb, fid=txt_fn.stem)\n",
    "        for idx, pred_s in enumerate(pred):\n",
    "            preds[pred_s[0]].append(pred_s)\n",
    "            \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_sent_id(en_pos, send_bound):\n",
    "    e_s = en_pos[0]\n",
    "    e_e = en_pos[1]\n",
    "    for k, v in sent_bound.items():\n",
    "        s_s = v[0]\n",
    "        s_e = v[1]\n",
    "        if e_s >= s_s and e_s <= s_e and e_e >s_e :\n",
    "            print(\"entity is in two sentence\")\n",
    "        if e_s >= s_s and e_s <= s_e:\n",
    "            return k\n",
    "        \n",
    "\n",
    "def extract_entity_comb_for_relation(e2idx, entities, rels, sent_bound):\n",
    "    #'T1': 0\n",
    "    #'meropenem', 'Drug', (4534, 4543)\n",
    "    #('Strength-Drug', 'T5', 'T39')\n",
    "    rn = defaultdict(list)\n",
    "    rl = []\n",
    "    for rel in rels:\n",
    "        rtype = rel[0]\n",
    "        en1 = rel[1]\n",
    "        en2 = rel[2]\n",
    "        en1_type = entities[e2idx[en1]][1]\n",
    "        en2_type = entities[e2idx[en2]][1]\n",
    "        rn[rtype].append((en1_type, en2_type))\n",
    "        en1_pos = entities[e2idx[en1]][2]\n",
    "        e1_n = en_sent_id(en1_pos, sent_bound)\n",
    "        en2_pos = entities[e2idx[en2]][2]\n",
    "        e2_n = en_sent_id(en2_pos, sent_bound)\n",
    "        rl.append(abs(e1_n-e2_n))\n",
    "    return rn, rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Output strategy:\n",
    "\n",
    "1. by cross-distance\n",
    "- no cross distance; all in one\n",
    "- by cross distance; all in unique\n",
    "- by partial cross distance; within-sentence vs. cross sentence\n",
    "\n",
    "2. relation format\n",
    "- [CLS] S1 [SEP] S2 [SEP]\n",
    "- [CLS] S1 S2 [SEP]\n",
    "\n",
    "We only handle (1) in data generation here, (2) will be handled by the data_utils\n",
    "\"\"\"\n",
    "def to_tsv(data, fn):\n",
    "    header = \"\\t\".join([str(i+1) for i in range(len(data[0]))])\n",
    "    with open(fn, \"w\") as f:\n",
    "        f.write(f\"{header}\\n\")\n",
    "        for each in data:\n",
    "            d = \"\\t\".join([str(e) for e in each])\n",
    "            f.write(f\"{d}\\n\")\n",
    "\n",
    "\n",
    "def to_5_cv(data, ofd):\n",
    "    if not os.path.isdir(ofd):\n",
    "        os.mkdir(ofd)\n",
    "    \n",
    "    np.random.seed(13)\n",
    "    np.random.shuffle(data)\n",
    "    \n",
    "    dfs = np.array_split(data, 5)\n",
    "    a = [0,1,2,3,4]\n",
    "    for each in combinations(a, 4):\n",
    "        b = list(set(a) - set(each))[0]\n",
    "        n = dfs[b]\n",
    "        m = []\n",
    "        for k in each:\n",
    "            m.extend(dfs[k])\n",
    "        if not os.path.isdir(os.path.join(ofd, f\"sample{b}\")):\n",
    "            os.mkdir(os.path.join(ofd, f\"sample{b}\"))\n",
    "        \n",
    "        to_tsv(m, os.path.join(ofd, f\"sample{b}\", \"train.tsv\"))\n",
    "        to_tsv(n, os.path.join(ofd, f\"sample{b}\", \"dev.tsv\"))\n",
    "\n",
    "\n",
    "def all_in_one(*dd, dn=\"2018n2c2\", do_train=True):\n",
    "    data = []\n",
    "    for d in dd:\n",
    "        for k, v in d.items():\n",
    "            for each in v:\n",
    "                data.append(each[1:])\n",
    "    \n",
    "    output_path = f\"../data/{dn}_aio_th{CUTOFF}\"\n",
    "    p = Path(output_path)\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if do_train:\n",
    "        to_tsv(data, p/\"train.tsv\")\n",
    "        if OUTPUT_CV:\n",
    "            to_5_cv(data, p.as_posix())\n",
    "    else:\n",
    "        to_tsv(data, p/\"test.tsv\")\n",
    "    \n",
    "\n",
    "def all_in_unique(*dd, dn=\"2018n2c2\", do_train=True):\n",
    "    for idx in range(CUTOFF+1):\n",
    "        data = []\n",
    "        for d in dd:\n",
    "            for k, v in d.items():\n",
    "                for each in v:\n",
    "                    if k == idx:\n",
    "                        data.append(each[1:])\n",
    "        \n",
    "        output_path = f\"../data/{dn}_aiu_th{CUTOFF}\"\n",
    "        p = Path(output_path) / f\"cutoff_{idx}\"\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "        if do_train:\n",
    "            to_tsv(data, p/\"train.tsv\")\n",
    "            if OUTPUT_CV:\n",
    "                to_5_cv(data, p.as_posix())\n",
    "        else:\n",
    "            to_tsv(data, p/\"test.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLOBAL CONFIG VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We did not automated this process\n",
    "You can manually create these variables\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general pre-defined special tags\n",
    "EN1_START = \"[s1]\"\n",
    "EN1_END = \"[e1]\"\n",
    "EN2_START = \"[s2]\"\n",
    "EN2_END = \"[e2]\"\n",
    "NEG_REL = \"NonRel\"\n",
    "# max valid cross sentence distance\n",
    "CUTOFF = 1\n",
    "# output 5-fold cross validation data\n",
    "OUTPUT_CV = False\n",
    "# do binary classification (if false, then we do multiclass classification)\n",
    "DO_BIN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid combination of entities as relations\n",
    "# only entity pairs with following type combinations will be included into training and test\n",
    "# other pairs will be excluded\n",
    "n2c2_valid_comb = {\n",
    "    ('ADE', 'Drug'), ('Reason', 'Drug'),\n",
    "    ('Strength', 'Drug'), ('Route', 'Drug'), \n",
    "    ('Frequency', 'Drug'), ('Dosage', 'Drug'),\n",
    "    ('Form', 'Drug'), ('Duration', 'Drug')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for binary classification\n",
    "# created based on valid combination\n",
    "# you need entp2rel for post-processing to generated Brat formatted output and perform evaluation\n",
    "\n",
    "entp2rel = {\n",
    "    ('ADE', 'Drug'):'ADE-Drug', \n",
    "    ('Reason', 'Drug'):'Reason-Drug',\n",
    "    ('Strength', 'Drug'):'Strength-Drug', \n",
    "    ('Route', 'Drug'):'Route-Drug', \n",
    "    ('Frequency', 'Drug'):'Frequency-Drug', \n",
    "    ('Dosage', 'Drug'):'Dosage-Drug',\n",
    "    ('Form', 'Drug'):'Form-Drug', \n",
    "    ('Duration', 'Drug'):'Duration-Drug'\n",
    "}\n",
    "\n",
    "if DO_BIN:\n",
    "    pkl_save(entp2rel, \"./data/2018n2c2_relation_processed/2018n2c2_mapping.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batchfy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2c2_test = \"./data/2018_n2c2_ade/gold_standard_test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many files you want to have in one directory\n",
    "BATCH_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fids = list(Path(n2c2_test).glob(\"*.txt\"))\n",
    "print(len(fids))\n",
    "fids_batchs = np.array_split(fids, len(fids)//BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_samples_by_batch(batch_fids, batch_id, valid_comb=None):\n",
    "    #create a separate mapping file\n",
    "    rel_mappings = []\n",
    "    preds = defaultdict(list)\n",
    "    \n",
    "    for txt_fn in batch_fids:\n",
    "        ann_fn = \"{}/{}{}\".format(txt_fn.parent, txt_fn.stem, \".ann\")\n",
    "        \n",
    "        txt = load_text(txt_fn)\n",
    "        pre_txt, sents = pre_processing(txt_fn, deid_pattern=MIMICIII_PATTERN)\n",
    "        e2i, ens, _ = read_annotation_brat(ann_fn)\n",
    "        i2e = {v: k for k, v in e2i.items()}\n",
    "        \n",
    "        nsents, sent_bound = generate_BIO(sents, ens, file_id=\"\", no_overlap=False, record_pos=True)\n",
    "        total_len = len(nsents)\n",
    "        nnsents = [w for sent in nsents for w in sent]\n",
    "        mappings = create_entity_to_sent_mapping(nnsents, ens, i2e)\n",
    "        \n",
    "        perm_pairs = get_permutated_relation_pairs(e2i)\n",
    "        pred = gene_neg_relation(perm_pairs, set(), mappings, ens, e2i, nnsents, nsents, valid_comb, fid=txt_fn.stem)\n",
    "        for idx, pred_s in enumerate(pred):\n",
    "            preds[pred_s[0]].append(pred_s)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_merged_into_one_dataset(*dd, output_path, batch_idx):\n",
    "    data = []\n",
    "    for d in dd:\n",
    "        for k, v in d.items():\n",
    "            for each in v:\n",
    "                data.append(each[1:])\n",
    "    \n",
    "    findal_output_path = output_path / f\"batch_{batch_idx}_aio_th{CUTOFF}\"\n",
    "    findal_output_path.mkdir(parents=True, exist_ok=True)\n",
    "    to_tsv(data, findal_output_path/\"test.tsv\")\n",
    "    \n",
    "\n",
    "def output_split_by_sent_cross_dis(*dd, output_path, batch_idx):\n",
    "    for idx in range(CUTOFF+1):\n",
    "        data = []\n",
    "        for d in dd:\n",
    "            for k, v in d.items():\n",
    "                for each in v:\n",
    "                    if k == idx:\n",
    "                        data.append(each[1:])\n",
    "                        \n",
    "        final_output_path = output_path / f\"batch_{batch_idx}_aiu_th{CUTOFF}/cutoff_{idx}\"\n",
    "        final_output_path.mkdir(parents=True, exist_ok=True)\n",
    "        to_tsv(data, final_output_path/\"test.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_path = Path(\"./data\")\n",
    "\n",
    "for batch_id, fids_batch in enumerate(fids_batchs):\n",
    "    preds = create_test_samples_by_batch(fids_batch, batch_id, n2c2_valid_comb)\n",
    "    \n",
    "    output_merged_into_one_dataset(preds, output_path=output_path/\"aio\", batch_idx=batch_id)\n",
    "    output_split_by_sent_cross_dis(preds, output_path=output_path/\"aiu\", batch_idx=batch_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output batch fild ids\n",
    "d = dict()\n",
    "for batch_id, fids_batch in enumerate(fids_batchs):\n",
    "    d[batch_id] = [e.as_posix() for e in fids_batch]\n",
    "\n",
    "with open(output_path/\"batch_file_ids.json\", \"w\") as f:\n",
    "    json.dump(d, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(output_path.glob(\"batch_*\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
